{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3EUuNxHrgXNR"
      },
      "outputs": [],
      "source": [
        "#import needed packages\n",
        "import numpy as np\n",
        "import os \n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import keras\n",
        "#import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, LeakyReLU, Reshape, Flatten, Input, Conv2D, MaxPooling2D, Activation, Dropout, Conv2DTranspose, GlobalAveragePooling2D, BatchNormalization\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on architecture from AnimeGAN and Ganyu\n",
        "# Function to list all images in a given directory\n",
        "def listImages(basePath, contains=None):\n",
        "   # Call listFiles function to list all files in the directory\n",
        "   return listFiles(basePath, contains=contains)\n",
        "\n",
        "              "
      ],
      "metadata": {
        "id": "o2mbYw2fgbSZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to list all files in a given directory\n",
        "def listFiles(basePath, contains=None):\n",
        "    # Iterate through the directory\n",
        "    for (rootDirectory, directoryNames, filenames) in os.walk(basePath):\n",
        "        # Iterate through the filenames\n",
        "        for f in filenames:\n",
        "            # Check if the filename contains the given string\n",
        "            if f.find(contains) == -1 and contains is not None:\n",
        "                # Skip the file if it does not contain the given string\n",
        "                continue\n",
        "          \n",
        "            yield os.path.join(rootDirectory, f).replace(\" \", \"\\\\ \")"
      ],
      "metadata": {
        "id": "RiLkueEDmDnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load images from a given directory\n",
        "def loadImages(directory='', size=(512,512)):\n",
        "    # Initialize an empty list for labels\n",
        "    labels, label, numToUse = [], 0, 10000\n",
        "    # Get a list of image paths\n",
        "    imagePaths = list(listImages(directory))[:numToUse]\n",
        "    # Initialize an empty list for images to be stored in\n",
        "    images = []\n",
        "    # Iterate through the image paths\n",
        "    for i in range(len(imagePaths)):\n",
        "      # Read and resize image, convert the image to RGB\n",
        "      images.append(cv2.cvtColor(cv2.resize(cv2.imread(imagePaths[i].replace('\\\\','/')), size), cv2.COLOR_BGR2RGB))\n",
        "    # Return the list of images\n",
        "    return images"
      ],
      "metadata": {
        "id": "cu0RZHR3mFjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attribution: \n",
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.img_shape = (512, 512, 3)\n",
        "        self.noise_size = 200\n",
        "        optimizer = Adam(0.0002,0.5)\n",
        "\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy', \n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "\n",
        "        self.generator = self.build_generator()\n",
        "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        \n",
        "        self.combined = Sequential()\n",
        "        self.combined.add(self.generator)\n",
        "        self.combined.add(self.discriminator)\n",
        "        \n",
        "        self.discriminator.trainable = False\n",
        "        \n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        \n",
        "        self.combined.summary()\n",
        "        \n",
        "    def build_generator(self):\n",
        "        noise_shape = (self.noise_size,)\n",
        "        \n",
        "        model = Sequential()\n",
        "        \n",
        "        model.add(Dense(4*4*512, activation='linear', input_shape=noise_shape))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((4, 4, 512)))\n",
        "        \n",
        "        model.add(Conv2DTranspose(1024, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n",
        "                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "         \n",
        "        model.add(Conv2DTranspose(1024, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n",
        "                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        \n",
        "        model.add(Conv2DTranspose(512, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n",
        "                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        \n",
        "        model.add(Conv2DTranspose(128, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n",
        "                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        \n",
        "        model.add(Conv2DTranspose(64, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n",
        "                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        \n",
        "        model.add(Conv2DTranspose(32, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n",
        "                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        \n",
        "        model.add(Conv2DTranspose(16, kernel_size=[4,4], strides=[2,2], padding=\"same\",\n",
        "                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        \n",
        "        model.add(Conv2DTranspose(3, kernel_size=[4,4], strides=[1,1], padding=\"same\",\n",
        "                                  kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "\n",
        "        model.add(Activation(\"tanh\"))\n",
        "        \n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=noise_shape)\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(128, (3,3), padding='same', input_shape=self.img_shape))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv2D(128, (3,3), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Conv2D(64, (3,3), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv2D(64, (3,3), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Conv2D(32, (3,3), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv2D(32, (3,3), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(64))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        \n",
        "        model.summary()\n",
        "        \n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "        \n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, metrics_update=50, save_images=100, save_model=2000):\n",
        "\n",
        "        X_train = np.array(images)\n",
        "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "        half_batch = int(batch_size / 2)\n",
        "        \n",
        "        mean_d_loss=[0,0]\n",
        "        mean_g_loss=0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (half_batch, self.noise_size))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "            d_loss = 0.5 * np.add(self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1))),\n",
        "                                  self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1))))\n",
        "\n",
        "            for _ in range(2):\n",
        "                noise = np.random.normal(0, 1, (batch_size, self.noise_size))\n",
        "\n",
        "                valid_y = np.array([1] * batch_size)\n",
        "                g_loss = self.combined.train_on_batch(noise, valid_y)\n",
        "            \n",
        "            mean_d_loss[0] += d_loss[0]\n",
        "            mean_d_loss[1] += d_loss[1]\n",
        "            mean_g_loss += g_loss\n",
        "            \n",
        "            if epoch % metrics_update == 0:\n",
        "                print (\"%d [Discriminator loss: %f, acc.: %.2f%%] [Generator loss: %f]\" % (epoch, mean_d_loss[0]/metrics_update, 100*mean_d_loss[1]/metrics_update, mean_g_loss/metrics_update))\n",
        "                mean_d_loss=[0,0]\n",
        "                mean_g_loss=0\n",
        "            \n",
        "            if epoch % save_images == 0:\n",
        "                self.save_images(epoch)\n",
        "            \n",
        "            if epoch % save_model == 0:\n",
        "                self.generator.save(\"generator_%d\" % epoch)\n",
        "                self.discriminator.save(\"discriminator_%d\" % epoch)\n",
        "\n",
        "    def save_images(self, epoch):\n",
        "        noise = np.random.normal(0, 1, (25, self.noise_size))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "        \n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(5,5, figsize = (30,30))\n",
        "\n",
        "        for i in range(5):\n",
        "            for j in range(5):\n",
        "                axs[i,j].imshow(gen_imgs[5*i+j])\n",
        "                axs[i,j].axis('off')\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "        fig.savefig(\"Boats/Boat_%d.png\" % epoch, dpi=200)\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "76sMWnT2gg4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images=loadImages('/input/boatpics/boats512')\n",
        "gan=GAN()\n",
        "gan.train(epochs=15001, batch_size=4, metrics_update=200, save_images=500, save_model=1000)"
      ],
      "metadata": {
        "id": "ILgFcAPPgpm0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}